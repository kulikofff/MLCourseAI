{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info from https://xgboost.readthedocs.io/en/stable/tutorials/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in e:\\kulikov\\ml\\mlcourseai\\mlcourseai\\.venv\\lib\\site-packages (1.7.4)\n",
      "Requirement already satisfied: numpy in e:\\kulikov\\ml\\mlcourseai\\mlcourseai\\.venv\\lib\\site-packages (from xgboost) (1.24.2)\n",
      "Requirement already satisfied: scipy in e:\\kulikov\\ml\\mlcourseai\\mlcourseai\\.venv\\lib\\site-packages (from xgboost) (1.10.1)\n"
     ]
    }
   ],
   "source": [
    "# For my Python 3.11 venv\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. GETTING STARTED\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "# read data\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['data'], data['target'], test_size=.2)\n",
    "# create model instance\n",
    "bst = XGBClassifier(n_estimators=2, max_depth=2, learning_rate=1, objective='binary:logistic')\n",
    "# fit model\n",
    "bst.fit(X_train, y_train)\n",
    "# make predictions\n",
    "preds = bst.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Introduction to Model IO\n",
    "#Shift+Alt+F for normal view in VSC\n",
    "bst.save_model('model_file_name.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0, -1,  0,  0], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds-y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.975"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_acc = bst.score(X_train, y_train)\n",
    "model_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DART BOOSTER\n",
    "# XGBoost mostly combines a huge number of regression trees with a small learning rate. In this situation, trees added early are significant and trees added late are unimportant.\n",
    "# It's a new method to add dropout techniques from the deep neural net community to boosted trees, and reported better results in some situations.\n",
    "# Features:\n",
    "# - Drop trees in order to solve the over-fitting.\n",
    "# - Trivial trees (to correct trivial errors) may be prevented.\n",
    "\n",
    "import xgboost as xgb\n",
    "# read in data\n",
    "#d_train = xgb.DMatrix('./agaricus.txt.train')\n",
    "#d_test = xgb.DMatrix('./agaricus.txt.test')\n",
    "\n",
    "# DMatrix is the basic data storage for XGBoost used by all XGBoost algorithms including both training, prediction and explanation. There are a few\n",
    "# variants of DMatrix including normal DMatrix, which is a CSR matrix, QuantileDMatrix, which is used by histogram-based tree methods for saving memory,\n",
    "# and lastly the experimental external-memory-based DMatrix, which reads data in batches during training. \n",
    "# NB! XGBoost DMatrix will blindly use the default LIBSVM parser. For CSV files, users need to provide an URI in the form of train.csv?format=csv\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, y_train)\n",
    "d_test = xgb.DMatrix(X_test, y_test)\n",
    "\n",
    "# specify parameters via map\n",
    "params = {'booster': 'dart',\n",
    "         'max_depth': 5, 'learning_rate': 0.1,\n",
    "         'num_class': 3, 'objective': 'multi:softmax', #for multicalss classification\n",
    "         'sample_type': 'uniform',\n",
    "         'normalize_type': 'tree',\n",
    "         'rate_drop': 0.1,\n",
    "         'skip_drop': 0.5}\n",
    "num_round = 50\n",
    "bst_dart = xgb.train(params, d_train, num_round)\n",
    "preds_dart = bst_dart.predict(d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Works fine:)\n",
    "preds_dart-y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standalone Random Forest  https://xgboost.readthedocs.io/en/stable/tutorials/rf.html\n",
    "# We can use XGBoost to train a standalone random forest or use random forest as a base model for gradient boosting with the following params:\n",
    "\n",
    "params = {\n",
    "  'colsample_bynode': 0.8,\n",
    "  'learning_rate': 1,\n",
    "  'max_depth': 5,\n",
    "  'num_parallel_tree': 100,\n",
    "  'objective': 'binary:logistic',\n",
    "  'subsample': 0.8,\n",
    "  'tree_method': 'gpu_hist'\n",
    "}\n",
    "\n",
    "bst = train(params, dmatrix, num_boost_round=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature interaction constraints.\n",
    "# \n",
    "# It allows users to decide which variables are allowed to interact and which are not.\n",
    "# Potential benefits include:\n",
    "#   Better predictive performance from focusing on interactions that work – whether through domain specific knowledge or algorithms that rank interactions\n",
    "#   Less noise in predictions; better generalization\n",
    "#   More control to the user on what the model can fit. For example, the user may want to exclude some interactions even if they perform well due to regulatory constraints.\n",
    "\n",
    "# For example, the constraint [0, 1] indicates that variables X0 and X1 are allowed to interact with each other but with no other variable. \n",
    "\n",
    "params_constrained = params.copy()\n",
    "# Use nested list to define feature interaction constraints\n",
    "params_constrained['interaction_constraints'] = '[[0, 1], [2, 3]]'\n",
    "# Features 0 and 2 are allowed to interact with each other but with no other feature\n",
    "# Features 1, 3, 4 are allowed to interact with one another but with no other feature\n",
    "# Features 5 and 6 are allowed to interact with each other but with no other feature\n",
    "\n",
    "model_with_constraints = xgb.train(params_constrained, d_train,\n",
    "                                   num_boost_round = 1000)\n",
    "model_with_constraints.save_model('model_with_constraints.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_constr = model_with_constraints.predict(d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0., -1.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_constr-y_test\n",
    "#Difference appered - it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB! XGBoost’s Python package supports using feature names instead of feature index for specifying the constraints. \n",
    "# Given a data frame with columns [\"f0\", \"f1\", \"f2\"], the feature interaction constraint can be specified as [[\"f0\", \"f2\"]]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Survival analysis\n",
    "\n",
    "# Survival analysis (regression) models time to an event of interest. Survival analysis is a special kind of regression and differs from the conventional regression task as follows:\n",
    "# The label is always positive, since you cannot wait a negative amount of time until the event occurs.\n",
    "# The label may not be fully known, or censored, because “it takes time to measure time.”\n",
    "# For example, it helps to works with infinity in target values\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "# 4-by-2 Data matrix\n",
    "X = np.array([[1, -1], [-1, 1], [0, 1], [1, 0]])\n",
    "dtrain = xgb.DMatrix(X)\n",
    "\n",
    "# Associate ranged labels with the data matrix.\n",
    "# This example shows each kind of censored labels.\n",
    "#                         uncensored    right     left  interval\n",
    "y_lower_bound = np.array([      2.0,     3.0,     0.0,     4.0])\n",
    "y_upper_bound = np.array([      2.0, +np.inf,     4.0,     5.0])\n",
    "dtrain.set_float_info('label_lower_bound', y_lower_bound)\n",
    "dtrain.set_float_info('label_upper_bound', y_upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-aft-nloglik:2.30142\n",
      "[1]\ttrain-aft-nloglik:2.24184\n",
      "[2]\ttrain-aft-nloglik:2.18633\n",
      "[3]\ttrain-aft-nloglik:2.13462\n",
      "[4]\ttrain-aft-nloglik:2.08645\n"
     ]
    }
   ],
   "source": [
    "#invoke the training API:\n",
    "#Note that it is not yet possible to set the ranged label using the scikit-learn interface (e.g. xgboost.XGBRegressor). For now, you should use xgboost.train with xgboost.DMatrix\n",
    "\n",
    "params = {'objective': 'survival:aft', # for this task\n",
    "          'eval_metric': 'aft-nloglik', # for this task\n",
    "          'aft_loss_distribution': 'normal', # for this task\n",
    "          'aft_loss_distribution_scale': 1.20, # for this task\n",
    "          'tree_method': 'hist', 'learning_rate': 0.05, 'max_depth': 2}\n",
    "bst_surv = xgb.train(params, dtrain, num_boost_round=5,\n",
    "                evals=[(dtrain, 'train')])\n",
    "\n",
    "bst_surv.save_model('bst_surv.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter Tuning\n",
    "\n",
    "# 1. Control overfitting parameters:\n",
    "    \n",
    "# - max_depth, min_child_weight and gamma.\n",
    "# - subsample and colsample_bytree.\n",
    "# - eta. Remember to increase num_round when you do so.\n",
    "\n",
    "# 2. Faster training performance\n",
    "# - tree_method, set it to hist or gpu_hist for faster computation.\n",
    "\n",
    "# 2. Handle Imbalanced Dataset\n",
    "\n",
    "# - Balance the positive and negative weights via scale_pos_weight. It is counted = sum(negative instances) / sum(positive instances)\n",
    "# - Use AUC for evaluation\n",
    "# - If you care about predicting the right probability Set parameter max_delta_step to a finite number (say 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "288df7f25e36581787dd8e5ac5724da1da4b04fd97b34615f5cfffeb90a4c951"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
